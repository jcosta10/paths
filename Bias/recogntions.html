<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
	<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0" />
	<title>Recognitions</title>
</head>
<body>
	
	<header>
	<a class="btn-icon" href="journey.html">
		<span class="material-symbols-outlined">close</span>
	</a>
	</header>
	
	<div class="go-to">
		<div class="line-back-second"></div>	
		<a href="discrimination.html" class="btn-right-back">[Discrimination]&#8963;</a>
	</div>	

	<div class="content">
		<h2 class="h2-button">Recognitions</h2> 
		<p class="p-left">> "Joy Buolamwini and computer scientist Timnit Gebrurevealed that facial recognition technology (FRT) has <a class="a-button" href="discrimination.html">difficulty identifying [<]</a> the gender of darker-skinned subjects. The problem stems from the libraries on which these 
			<span class="container-legend">
				<span class="more">algorithms[+] 
					<span class="legend-more"> 
					 “a set of mathematical instructions or rules that, especially if given to a computer, will help to calculate an answer to a problem” [©: Cambridge Dictionary (2022). University Press]</span>
				</span>
			</span> have been traditionally trained: the “ground truth” for these programs are the faces of Hollywood celebrities 
		<span class="container-img">
			<span class="tooltip">[❑]
				<span class="image1">
				<img src="Face.png">
				<span class="p-legend">[❑] Figure 5. Faces generated using the faces of Hollywood celebrities.Progressive Growing of GANs.</span>
				</span>
			</span> 
		</span> 
		and university undergraduates, those well-known hotspots of 
			<span class="container-legend">
				<span class="more">diversity[+] 
					<span class="legend-more"> 
					 “the fact of many different types of things or people being included in something; a range of different things or people” [©: Cambridge Dictionary (2022). University Press]</span>
				</span>
			</span>. At a fundamental level, this “curation” means that <a class="a-button" href="debiasing.html">ground truth = deep fake [>]</a>ground truth = deep fake." 
			<span class="container-legend">
				<span class="legend">[©:3]
					<span class="legend-1">[©:3] Chun, Wendy. 2021. Discriminating Data: Correlation, Neighborhoods, and the New Politics of Recognition. Cambridge, Massachusetts: The MIT Press.</span>
				</span>
			</span>
		</p>
		<p class="p-left">> "Keyes’s study of automatic gender detection in facial recognition shows that almost 95 percent of papers in the field treat gender as binary, with the majority describing gender as immutable and 
			<span class="container-legend">
				<span class="more">physiological[+] 
					<span class="legend-more"> 
					 “relating to the way in which the bodies of living things work” [©: Cambridge Dictionary (2022). University Press]</span>
				</span>
			</span>. While some might respond that this can be easily remedied by creating more categories, this 
			<span class="container-legend">
				<span class="more">fails[+] 
					<span class="legend-more"> 
					 “to not succeed in what you are trying to achieve or are expected to do” [©: Cambridge Dictionary (2022). University Press]</span>
				</span>
			</span> to address the deeper harm of allocating people into gender or race categories without their input or consent. This practice has a long history. Administrative systems for centuries have sought to make humans legible by applying fixed labels and definite properties. The work of essentializing and ordering 
			<span class="container-legend">
				<span class="more">on the basis of[+] 
					<span class="legend-more"> 
					 “a way or method of doing something” [©: Cambridge Dictionary (2022). University Press]</span>
				</span>
			</span> biology or culture has long been used to justify forms of violence and oppression."
			<span class="container-legend">
				<span class="legend">[©:2]
					<span class="legend-2">[©:2] Crawford, Kate. 2021 Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. New Haven, London: Yale University Press.</span>
				</span>
			</span>
		</p>
		<div>
			<div class="line-down-left"></div>	
			<a href="debiasing.html" class="btn-after-left">[Debiasing]&#8964; </a>
		</div>	
		
	</div>

</body>
</html>